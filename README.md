# soft-vc-acoustic-model-ablation-study

    Ablation study on acoustic model of Soft-VC, aming to comprehend and distillate/simplify it.

----

åŸºç¡€ä»£ç ç”± [soft-vc-acoustic-models](https://github.com/Kahsolt/soft-vc-acoustic-models) åˆ å‡ä¿®æ”¹è€Œæ¥ï¼Œè¿›ä¸€æ­¥åˆ å»äº†å¹¶è¡Œè®­ç»ƒæ¡†æ¶ï¼Œå› ä¸ºæˆ‘åªæœ‰ä¸€å¼ å¡è°”è°” :( 


### Ablation Study Results

We mainly use the standard datasets [DataBaker](https://www.data-baker.com/data/index/TNtts/) (Mandarin) for the following experiments, 
you could test everything alike on [LJSpeech](https://keithito.com/LJ-Speech-Dataset/) (English) by yourself once you have more powerful GPUs :) 

| è¯­æ–™ | æ•°æ®é›†(dataset) | è¯´æ˜ | æ•°æ®é›†æ—¶é•¿ |
| :-: | :-: | :-: | :-: |
| DataBaker(BZNSYP) | databaker | æ±‰è¯­æ™®é€šè¯å¥³æ€§æˆäºº | 10h |
| LJSpeech-1.1      | databaker | è‹±è¯­å¥³æ€§æˆäºº      | 24h |

You can listen to the audio samples in `index.html`, checkponts could also be downloaded from here [https://pan.quark.cn/s/48dcffa2cddf](https://pan.quark.cn/s/48dcffa2cddf)

#### Ablation on size of training data

Now concerning about **how many data is necessarily needed** to train a satisfactory voice bank.  
We tested the following settings:

| è®­ç»ƒåˆ—è¡¨(list) | è®­ç»ƒ-éªŒè¯é›†æ€»æ—¶é•¿ |
| :-: | :-: |
| databaker-full  | 10h (full) |
| databaker-8h    | 8h         |
| databaker-4h    | 4h         |
| databaker-2h    | 2h         |
| databaker-1h    | 1h         |
| databaker-30min | 30min      |
| databaker-10min | 10min      |

#### Ablation on model architecture

Now concerning about **what each component functions** in the original soft-vc acoustic model. 
We tested these modified version together with the original CRNN-based acoustic model:

| æ¨¡å‹(model) | å‚æ•°é‡| è¯´æ˜ |
| :-: | :-: | :-: |
| baseline     | 18,830,336 | original version |
| no_dropout   | 18,830,336 | PreNets do not Dropout in both Encoder and Decoder |
| no_IN        | 18,830,336 | no InstanceNorm1d in Encoder |
| single_LSTM  |  9,380,864 | use only one RNN layer in Decoder |
| only_Encoder |  4,524,544 | only CNN (- IN), no RNN; loss does **NOT** decrease if apply `IN` |
| only_Decoder | 14,896,128 | only (Conv1dT +) RNN , no CNN; `Conv1dT` is added to upsample for length match |
| tiny         |  7,151,104 | tiny version of baseline architecture, collapses all replicated layers/blocks |
| tiny_half    |  2,051,968 | tiny, but model width halfed to `384` |

![Acoustic Model Zoo](img/model_zoo.png)

#### Statistic Results

Collected from demo experiments in `run_experiments.cmd`

| Experiment Setting | Best ckpt Steps | Final Loss (train / valid) | Note | Listening Test |
| :-: | :-: | :-: | :-: | :-: |
| baseline_databaker-full     | 23000 | 0.31 / 0.33745 | æ•°æ®é›†å¤ªå°ï¼Œæ¨¡å‹å¤ªå¤§ | ä¸ªåˆ«å£°è°ƒé”™è¯¯ |
| baseline_databaker-8h       | 23000 | 0.30 / 0.35051 | æ•°æ®é›†å¤ªå°ï¼Œæ¨¡å‹å¤ªå¤§ | è·¨è¯­ç§æ—¶ä¸ªåˆ«éŸ³ç´ ç¼ºå¤± |
| baseline_databaker-4h       | 15000 | 0.27 / 0.38186 | æ•°æ®é›†å¤ªå°ï¼Œæ¨¡å‹å¤ªå¤§ | ä¸ªåˆ«éŸ³ç´ ç¼ºå¤±å¯¼è‡´å™ªå£°ï¼Œå£°è°ƒé”™è¯¯ |
| baseline_databaker-2h       | 5000  | 0.23 / 0.41324 | å¿«é€Ÿè¿‡æ‹Ÿåˆï¼Œ**æ•°æ®é‡éœ€æ±‚ä¸‹ç•Œ** | ä¸ªåˆ«éŸ³ç´ ç¼ºå¤±ï¼Œå£°è°ƒé”™è¯¯ |
| baseline_databaker-1h       | 4000  | 0.18 / 0.43975 | å¿«é€Ÿè¿‡æ‹Ÿåˆ | éŸ³ç´ ç¼ºå¤±å¯¼è‡´è¿‘éŸ³æ›¿æ¢ |
| baseline_databaker-30min    | 3000  | 0.12 / 0.50549 | å¿«é€Ÿè¿‡æ‹Ÿåˆ | éŸ³ç´ ç¼ºå¤±ï¼Œå£°è°ƒé”™è¯¯ï¼Œå£°éŸ³æ’•è£‚ |
| baseline_databaker-10min    | 1000  | 0.06 / 0.58868 | å¿«é€Ÿè¿‡æ‹Ÿåˆ | éŸ³ç´ ç¼ºå¤±å¯¼è‡´ç™½å™ªéŸ³æ›¿æ¢ï¼Œæ’•è£‚ |
| no_dropout_databaker-full   | 35000 | 0.17 / 0.18310 | ï¼Ÿæˆ‘ä¸ç†è§£ï¼Œlossæœ€ä½å´ä»æœªè¿‡æ‹Ÿåˆ | å£°éŸ³æ›´å•å£°é“ï¼Œæ³›éŸ³æ›´å°‘(é—·)ï¼›æ›´å°‘çš„æ±‰è¯­å£éŸ³ï¼Œä½†å¸¦æœ‰æºéŸ³è‰²æ¨¡å¼çš„æˆåˆ†ï¼ |
| no_IN_databaker-full        | 23000 | 0.31 / 0.33172 | HuBERTæœ¬èº«å°±æœ‰INçš„æ„å‘³ï¼Œå†åŠ INåªèƒ½é™ä¸€ç‚¹ç‚¹loss | æ±‰è¯­å£éŸ³ï¼ŒéŸ³è°ƒé”™è¯¯ |
| single_LSTM_databaker-full  | 28000 | 0.34 / 0.42488 | å•å±‚RNNä¼¼ä¹æ€»æ˜¯éçº¿æ€§æ€§ä¸å¤ªå¤Ÿï¼ŒéªŒè¯é›†lossä¸ç¨³ | å¬èµ·æ¥è¿˜å¯ä»¥ |
| only_Encoder_databaker-full | 36000 | 0.54 / 0.52248 | æ²¡RNNçš„è¯lossé™ä¸ä¸‹æ¥ï¼ŒéŸµå¾‹è¿ç§»è¦é RNNï¼ | å¾®å°çš„æ’•è£‚éŸ³ï¼ˆä»€ä¹ˆæ ¸å—“ï¼‰ |
| only_Decoder_databaker-full | 26000 | 0.31 / 0.33341 | CNNå¯¹äºé™lossè¿™ä»¶äº‹çœ‹æ¥ç”¨å¤„ä¸å¤§ | ä¸¥é‡çš„éŸ³è°ƒé”™è¯¯ï¼Œå¬èµ·æ¥åƒè‹±è¯­ğŸ¤”ï¼Ÿ |
| tiny_databaker-full         | 36000 | 0.18 / 0.18912 | **åŒ¹é…æ•°æ®é›†è§„æ¨¡çš„åˆç†è®¾ç½®** | å¬è§‰æ•ˆæœæœ€å¥½ |
| tiny_half_databaker-full    | 35000 | 0.19 / 0.20307 | å®½åº¦å¤ªå°éçº¿æ€§æ€§å¯èƒ½ä¸å¤Ÿ | å¾®å°çš„æ’•è£‚éŸ³å’Œé—´æ­‡çš„é«˜é¢‘å™ªå£° |


### Quick Start

#### preprocess

0. download and unzip the dataset [DataBaker](https://www.data-baker.com/data/index/TNtts/) 
1. install dependencies: `pip install -r requirements.txt`
2. perform data preprocess: `mk_preprocess.cmd databaker C:\BZNSYP\Wave`, feature data are generated under `data` folder
3. make training list files: `python make_lists.py databaker`, list files are generated under `lists` folder

#### experiments on size of training data

train using different training list like (by default use `--model baseline`):

  - `python train.py databaker --listfp lists\databaker-full.txt`
  - `python train.py databaker --listfp lists\databaker-1h.txt`
  - `python train.py databaker --listfp lists\databaker-10min.txt`
  - see `lists` folder for all available training list

checkponts and logs are located at `log\<model>-<list>` (e.g. `log\baseline_databaker-1h`)

#### experiments on model architecture

train any acoustic model in the model zoo like

  - `python train.py databaker --listfp lists\databaker-full.txt --model baseline`
  - `python train.py databaker --listfp lists\databaker-full.txt --model no_IN`
  - `python train.py databaker --listfp lists\databaker-full.txt --model single_LSTM`
  - see `python train.py --help` for all available models

checkponts and logs are located at `log\<model>-<list>` (e.g. `log\single_LSTM_databaker-full`)

#### make table view for convenient comparison

1. run `make_infer_test.cmd` to do voice conversion over all trained models on all wavfiles under the `test` folder
2. run `make_index.py` to generated `index.html`, then open it in your browser, now you can check the quality of synthezied results in the table grid


### References

Great thanks again to the founding authors of Soft-VC! :lollipop:  
And also the open-source dataset provider!  :)  

```
@inproceedings{
  soft-vc-2022,
  author={van Niekerk, Benjamin and Carbonneau, Marc-AndrÃ© and ZaÃ¯di, Julian and Baas, Matthew and SeutÃ©, Hugo and Kamper, Herman},
  booktitle={ICASSP}, 
  title={A Comparison of Discrete and Soft Speech Units for Improved Voice Conversion}, 
  year={2022}
}
```

- soft-vc paper: [https://ieeexplore.ieee.org/abstract/document/9746484](https://ieeexplore.ieee.org/abstract/document/9746484)
- soft-vc code: [https://github.com/bshall/soft-vc](https://github.com/bshall/soft-vc)
  - hubert: [https://github.com/bshall/hubert](https://github.com/bshall/hubert)
  - acoustic-model: [https://github.com/bshall/acoustic-model](https://github.com/bshall/acoustic-model)
  - hifigan: [https://github.com/bshall/hifigan](https://github.com/bshall/hifigan)

----

by Armit
2022/09/12 
